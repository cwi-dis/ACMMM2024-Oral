{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py37 (Python 3.7.16)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py37 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.backbones import pointnet2, resnet50\n",
    "from models.transformer import TransformerEncoderLayer_CMA\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from models.Gdn import Gdn  # use this as a activation function\n",
    "import math\n",
    "\n",
    "\n",
    "class CMA_fusion(nn.Module):\n",
    "    def __init__(self, img_inplanes, pc_inplanes, cma_planes=1024, use_local=1):\n",
    "        super(CMA_fusion, self).__init__()\n",
    "        self.global_local_encoder = TransformerEncoderLayer_CMA(\n",
    "            d_model=cma_planes,\n",
    "            nhead=8,\n",
    "            img_inplanes=img_inplanes,\n",
    "            pc_inplanes=pc_inplanes,\n",
    "            cma_planes=cma_planes,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.use_local = use_local\n",
    "        self.linear1 = nn.Linear(img_inplanes, cma_planes)\n",
    "        self.linear2 = nn.Linear(pc_inplanes, cma_planes)\n",
    "        # xm: do the batch normalization for the input of the cross modal attention: shape = [B, pc_projection or pc_patch_number, cma_planes]\n",
    "        self.img_bn = nn.BatchNorm1d(cma_planes)\n",
    "        self.pc_bn = nn.BatchNorm1d(cma_planes)\n",
    "\n",
    "    def forward(self, texture_img, geometry_img, texture_pc, geometry_pc):\n",
    "        # linear mapping and batch normalization\n",
    "        texture_img = self.linear1(texture_img)\n",
    "        # change the shape of the input of the cross modal attention img\n",
    "        texture_img = texture_img.permute(0, 2, 1)\n",
    "        texture_img = self.img_bn(\n",
    "            texture_img\n",
    "        )  # xm: shape = [B, pc_projection, cma_planes]\n",
    "\n",
    "        # linear mapping and batch normalization for geometry map\n",
    "        geometry_img = self.linear1(geometry_img)\n",
    "        geometry_img = geometry_img.permute(0, 2, 1)\n",
    "        geometry_img = self.img_bn(geometry_img)\n",
    "\n",
    "        # linear mapping and batch normalization for pc texture\n",
    "        texture_pc = self.linear2(texture_pc)\n",
    "        # change the shape of the input of the cross modal attention pc for batch normalization\n",
    "        texture_pc = texture_pc.permute(0, 2, 1)\n",
    "        texture_pc = self.pc_bn(texture_pc)\n",
    "\n",
    "        # linear mapping and batch normalization for pc geometry\n",
    "        geometry_pc = self.linear2(geometry_pc)\n",
    "        geometry_pc = geometry_pc.permute(0, 2, 1)\n",
    "        geometry_pc = self.pc_bn(geometry_pc)\n",
    "\n",
    "        # change img and pc back to the original shape\n",
    "        texture_img = texture_img.permute(0, 2, 1)\n",
    "        geometry_img = geometry_img.permute(0, 2, 1)\n",
    "        texture_pc = texture_pc.permute(0, 2, 1)\n",
    "        geometry_pc = geometry_pc.permute(0, 2, 1)\n",
    "\n",
    "        (\n",
    "            tex_img_global,\n",
    "            tex_pc_global,\n",
    "            geometry_img_global,\n",
    "            geometry_pc_global,\n",
    "            tex2D_geo2D_attention,\n",
    "            tex2D_geo2D_global_attention,\n",
    "            geo2D_tex2D_attention,\n",
    "            geo2D_tex2D_global_attention,\n",
    "            tex3D_geo3D_attention,\n",
    "            tex3D_geo3D_global_attention,\n",
    "            geo3D_tex3D_attention,\n",
    "            geo3D_tex3D_global_attention,\n",
    "            tex2D_tex3D_attention,\n",
    "            tex2D_tex3D_global_attention,\n",
    "            tex3D_tex2D_attention,\n",
    "            tex3D_tex2D_global_attention,\n",
    "            tex2D_geo3D_attention,\n",
    "            tex2D_geo3D_global_attention,\n",
    "            geo3D_tex2D_attention,\n",
    "            geo3D_tex2D_global_attention,\n",
    "            geo2D_tex3D_attention,\n",
    "            geo2D_tex3D_global_attention,\n",
    "            tex3D_geo2D_attention,\n",
    "            tex3D_geo2D_global_attention,\n",
    "            geo2D_geo3D_attention,\n",
    "            geo2D_geo3D_global_attention,\n",
    "            geo3D_geo2D_attention,\n",
    "            geo3D_geo2D_global_attention,\n",
    "        ) = self.global_local_encoder(\n",
    "            texture_img, geometry_img, texture_pc, geometry_pc\n",
    "        )\n",
    "\n",
    "        output_local = torch.cat(\n",
    "            (\n",
    "                texture_img,\n",
    "                geometry_img,\n",
    "                texture_pc,\n",
    "                geometry_pc,  # orginal\n",
    "                tex2D_geo2D_attention,\n",
    "                geo2D_tex2D_attention,\n",
    "                tex3D_geo3D_attention,\n",
    "                geo3D_tex3D_attention,\n",
    "                tex2D_tex3D_attention,\n",
    "                tex3D_tex2D_attention,\n",
    "                tex2D_geo3D_attention,\n",
    "                geo3D_tex2D_attention,\n",
    "                geo2D_tex3D_attention,\n",
    "                tex3D_geo2D_attention,\n",
    "                geo2D_geo3D_attention,\n",
    "                geo3D_geo2D_attention,\n",
    "            ),\n",
    "            dim=1,\n",
    "        ).mean(\n",
    "            dim=1\n",
    "        )  #  keepdim=True # xm: shape = [B, cma_planes] after the mean operation, the shape of the output is [B, cma_planes]\n",
    "\n",
    "        output_global = torch.stack(  # xm: stack the tensors in a new dimension\n",
    "            (\n",
    "                tex_img_global,\n",
    "                tex_pc_global,\n",
    "                geometry_img_global,\n",
    "                geometry_pc_global,\n",
    "                tex2D_geo2D_global_attention,\n",
    "                geo2D_tex2D_global_attention,\n",
    "                tex3D_geo3D_global_attention,\n",
    "                geo3D_tex3D_global_attention,\n",
    "                tex2D_tex3D_global_attention,\n",
    "                tex3D_tex2D_global_attention,\n",
    "                tex2D_geo3D_global_attention,\n",
    "                geo3D_tex2D_global_attention,\n",
    "                geo2D_tex3D_global_attention,\n",
    "                tex3D_geo2D_global_attention,\n",
    "                geo2D_geo3D_global_attention,\n",
    "                geo3D_geo2D_global_attention,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        ).mean(dim=-1)\n",
    "\n",
    "        # output_global = output_global.squeeze(0)\n",
    "        if self.use_local:\n",
    "            output = output_local + output_global\n",
    "        else:\n",
    "            output = output_global\n",
    "        # output = torch.cat((output_local, output_global), dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class QualityRegression(nn.Module):\n",
    "    def __init__(self, cma_planes=1024):\n",
    "        super(QualityRegression, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.quality1 = nn.Linear(cma_planes, cma_planes // 2)\n",
    "        self.quality2 = nn.Linear(cma_planes // 2, 1)\n",
    "\n",
    "    def forward(self, fusion_output):\n",
    "        # mos regression # add the relu activation function\n",
    "        regression_output = self.activation(self.quality1(fusion_output))\n",
    "        regression_output = self.quality2(regression_output)\n",
    "        return regression_output\n",
    "\n",
    "\n",
    "class DistortionClassification(nn.Module):\n",
    "    def __init__(self, cma_planes=1024, num_classes=None, dropout_prob=0.5):\n",
    "        super(DistortionClassification, self).__init__()\n",
    "\n",
    "        self.classifier1 = nn.Linear(cma_planes, cma_planes // 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(\n",
    "            p=dropout_prob\n",
    "        )  # Dropout layer with the specified dropout probability\n",
    "        self.classifier2 = nn.Linear(cma_planes // 2, cma_planes // 4)\n",
    "        self.classifier3 = nn.Linear(cma_planes // 4, num_classes)\n",
    "\n",
    "    def forward(self, fusion_output):\n",
    "        classification_output = self.activation(self.classifier1(fusion_output))\n",
    "        classification_output = self.activation(self.classifier2(classification_output))\n",
    "        classification_output = self.dropout(\n",
    "            classification_output\n",
    "        )  # Applying dropout PQA-net add a batchnormal\n",
    "        classification_output = self.classifier3(classification_output)\n",
    "        return classification_output\n",
    "\n",
    "\n",
    "class MM_PCQAnet(nn.Module):\n",
    "    def __init__(self, num_classes, args):\n",
    "        super(\n",
    "            MM_PCQAnet,\n",
    "            self,\n",
    "        ).__init__()  # inherits all the functionalities and attributes of the nn.Module\n",
    "        self.img_inplanes = args.img_inplanes\n",
    "        self.pc_inplanes = args.pc_inplanes\n",
    "        self.cma_planes = args.cma_planes\n",
    "        self.use_local = args.use_local\n",
    "\n",
    "        self.img_backbone = resnet50(pretrained=True)\n",
    "        self.depth_backbone = resnet50(\n",
    "            pretrained=True\n",
    "        )  # xm: should I still set pretrained = True?\n",
    "        self.normal_backbone = resnet50(pretrained=True)\n",
    "        self.pc_position_backbone = pointnet2()\n",
    "        self.pc_normal_backbone = pointnet2()\n",
    "        self.pc_texture_backbone = pointnet2()\n",
    "        self.SharedFusion = CMA_fusion(\n",
    "            self.img_inplanes, self.pc_inplanes, self.cma_planes, self.use_local\n",
    "        )  # xm: img_inplanes = 2048 image feature hidden embedding, pc_inplanes = 1024 pc feature hidden embedding, cma_planes = 1024 cross modal attention hidden embedding\n",
    "        self.MosRegression = QualityRegression()\n",
    "        self.DistortionClassify = DistortionClassification(num_classes=num_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, texture_img, depth_img, normal_img, texture_pc, normal_pc, position_pc\n",
    "    ):\n",
    "        # extract features from the projections\n",
    "        img_size = texture_img.shape  # [B, N, C, Height, Width]\n",
    "\n",
    "        texture_img = texture_img.view(\n",
    "            -1, img_size[2], img_size[3], img_size[4]\n",
    "        )  # xm: reshaping input data to match the expected input format of a neural network model. -1 means the size of that dimension is inferred from the size of the tensor and the remaining dimensions\n",
    "        # xm shape: [B*N, C, Height, Width]\n",
    "        texture_img = self.img_backbone(texture_img)  # xm shape: [B*N, HiddenImg, 1, 1]\n",
    "        texture_img = torch.flatten(texture_img, 1)  # xm shape: [B*N, HiddenImg]\n",
    "        # average the projection features (xm: why first flatten and then view??)\n",
    "        texture_img = texture_img.view(\n",
    "            img_size[0], img_size[1], self.img_inplanes\n",
    "        )  # xm: [B, N, HiddenImg]\n",
    "\n",
    "        # shape: [B, HiddenImg] HiddenImg = 2048\n",
    "        # extract features from depths\n",
    "        depth = depth_img.view(-1, img_size[2], img_size[3], img_size[4])\n",
    "        depth = self.depth_backbone(depth)\n",
    "        depth = torch.flatten(depth, 1)\n",
    "        depth = depth.view(img_size[0], img_size[1], self.img_inplanes)\n",
    "\n",
    "        # extract features from normals\n",
    "        normal = normal_img.view(-1, img_size[2], img_size[3], img_size[4])\n",
    "        normal = self.normal_backbone(normal)\n",
    "        normal = torch.flatten(normal, 1)\n",
    "        normal = normal.view(img_size[0], img_size[1], self.img_inplanes)\n",
    "\n",
    "        texture_pc_size = (\n",
    "            texture_pc.shape\n",
    "        )  # [B, sub-models, Coords + normal channel number?，K]\n",
    "        texture_pc = texture_pc.view(\n",
    "            -1, texture_pc_size[2], texture_pc_size[3]\n",
    "        )  # [B*sub-models (BxM), Coords，K]\n",
    "        texture_pc = self.pc_texture_backbone(\n",
    "            texture_pc\n",
    "        )  # [B*M, HiddenPC] HiddenPC = 1024\n",
    "        # average the patch features\n",
    "        texture_pc = texture_pc.view(\n",
    "            texture_pc_size[0], texture_pc_size[1], self.pc_inplanes\n",
    "        )  # [B, M, HiddenPC]\n",
    "\n",
    "        normal_pc_size = normal_pc.shape\n",
    "        normal_pc = normal_pc.view(-1, normal_pc_size[2], normal_pc_size[3])\n",
    "        normal_pc = self.pc_normal_backbone(normal_pc)\n",
    "        normal_pc = normal_pc.view(\n",
    "            normal_pc_size[0], normal_pc_size[1], self.pc_inplanes\n",
    "        )\n",
    "\n",
    "        position_pc_size = position_pc.shape\n",
    "        position_pc = position_pc.view(-1, position_pc_size[2], position_pc_size[3])\n",
    "        position_pc = self.pc_position_backbone(position_pc)\n",
    "        position_pc = position_pc.view(\n",
    "            position_pc_size[0], position_pc_size[1], self.pc_inplanes\n",
    "        )\n",
    "        geometry_img = depth + normal\n",
    "        geometry_pc = normal_pc + position_pc\n",
    "\n",
    "        # TODO Before put them into CMA, we need to aline the shape of img_global and geometry_global\n",
    "        # attention, fusion, and regression and classification\n",
    "        fusion_output_local_global = self.SharedFusion(\n",
    "            texture_img,\n",
    "            geometry_img,\n",
    "            texture_pc,\n",
    "            geometry_pc,\n",
    "        )\n",
    "\n",
    "        output_regression = self.MosRegression(fusion_output_local_global)\n",
    "        output_classification = self.DistortionClassify(fusion_output_local_global)\n",
    "        return output_regression, output_classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
